{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43e3d50",
   "metadata": {},
   "source": [
    "## MLflow Registry: Gemini (Google GenAI) ‚Äî Minimal Notebook\n",
    "\n",
    "**Objetivo:** Validar *solo* la parte del **Model Registry** de MLflow con un modelo Chat que usa **Google GenAI (Gemini)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "031a72e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d78e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d45f187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"tracking_llm_real\")\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20be776",
   "metadata": {},
   "source": [
    "## Creaci√≥n del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cefaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Crear o usar un experimento para LLMs\n",
    "mlflow.set_experiment(\"llm_genai_model_registry\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528723cf",
   "metadata": {},
   "source": [
    "# Google model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ab83f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI son las siglas en ingl√©s de **Artificial Intelligence**, que en espa√±ol se traduce como **Inteligencia Artificial (IA)**.\\n\\nEn t√©rminos generales, la IA se refiere a la **capacidad de una m√°quina o sistema inform√°tico para imitar las capacidades cognitivas humanas**, como el aprendizaje, el razonamiento, la resoluci√≥n de problemas, la percepci√≥n (visual y auditiva), la comprensi√≥n del lenguaje natural y la toma de decisiones.\\n\\nEs importante destacar que la IA no es una sola cosa, sino un campo amplio y en constante evoluci√≥n que engloba diferentes t√©cnicas y enfoques.\\n\\nAqu√≠ te dejo una desglose de los aspectos clave de la IA:\\n\\n*   **Objetivo:** Crear m√°quinas que puedan realizar tareas que normalmente requieren inteligencia humana.\\n*   **T√©cnicas:** Se basa en una variedad de t√©cnicas, incluyendo el aprendizaje autom√°tico (machine learning), el aprendizaje profundo (deep learning), el procesamiento del lenguaje natural (NLP), la visi√≥n artificial (computer vision), la rob√≥tica, y los sistemas expertos.\\n*   **Aplicaciones:** Tiene una amplia gama de aplicaciones en diversos campos, como la medicina, la banca, el transporte, la manufactura, el marketing, la educaci√≥n, el entretenimiento, y la seguridad.\\n*   **Tipos:** Se puede clasificar en diferentes tipos, como:\\n    *   **IA D√©bil (o IA Estrecha):** Dise√±ada para realizar una tarea espec√≠fica, como jugar al ajedrez o reconocer rostros.\\n    *   **IA Fuerte (o IA General):** Te√≥ricamente capaz de entender, aprender y aplicar su inteligencia a cualquier tarea que un ser humano pueda hacer. (Esta a√∫n no existe).\\n*   **Aprendizaje Autom√°tico (Machine Learning):** Una rama de la IA que permite a las m√°quinas aprender de los datos sin ser programadas expl√≠citamente.  Es decir, en lugar de darle instrucciones espec√≠ficas para cada situaci√≥n, la m√°quina aprende patrones y reglas a partir de los datos que se le proporcionan.\\n*   **Aprendizaje Profundo (Deep Learning):**  Un tipo de aprendizaje autom√°tico que utiliza redes neuronales artificiales con m√∫ltiples capas para analizar datos complejos y extraer caracter√≠sticas relevantes.  Ha sido muy exitoso en tareas como el reconocimiento de im√°genes y el procesamiento del lenguaje natural.\\n\\n**En resumen, la IA es un campo de la inform√°tica dedicado a crear sistemas inteligentes capaces de realizar tareas que requieren inteligencia humana.  Es una tecnolog√≠a con un enorme potencial para transformar la sociedad en el futuro.**\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "gemini = OpenAI(api_key=gemini_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Que es AI\"\n",
    "    }\n",
    "  ])\n",
    "answer = response.choices[0].message.content\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8e7af46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run gemini_tracking at: http://127.0.0.1:5000/#/experiments/539662435075150292/runs/d81a92db08e4437f811e9231e7e83dbd\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/539662435075150292\n",
      "‚úÖ GEMINI: La Inteligencia Artificial (IA) es un campo de la inform√°tica que se centra en la creaci√≥n de sistemas inform√°ticos capaces de realizar tareas que normalmente requieren inteligencia humana. En otras palabras, busca **simular la inteligencia humana en m√°quinas**.\n",
      "\n",
      "Aqu√≠ te desglosamos los aspectos clave de la IA:\n",
      "\n",
      "*   **Simulaci√≥n de la Inteligencia Humana:** La IA busca replicar o imitar habilidades cognitivas humanas como:\n",
      "    *   **Aprendizaje:** La capacidad de aprender de los datos y mejorar el rendimiento con la experiencia.\n",
      "    *   **Razonamiento:** La capacidad de resolver problemas, tomar decisiones y sacar conclusiones l√≥gicas.\n",
      "    *   **Percepci√≥n:** La capacidad de interpretar y comprender informaci√≥n sensorial (como im√°genes, sonido, texto).\n",
      "    *   **Resoluci√≥n de Problemas:** La capacidad de identificar y encontrar soluciones a problemas complejos.\n",
      "    *   **Comprensi√≥n del Lenguaje Natural:** La capacidad de entender y generar lenguaje humano.\n",
      "\n",
      "*   **Tipos de IA:**\n",
      "    *   **IA D√©bil o Estrecha (Narrow AI):** Dise√±ada para realizar una tarea espec√≠fica. La mayor√≠a de las IA que vemos hoy en d√≠a son IA estrechas (ej: reconocimiento facial, asistentes virtuales, sistemas de recomendaci√≥n).\n",
      "    *   **IA Fuerte o General (Artificial General Intelligence - AGI):**  Tiene la capacidad de entender, aprender, adaptar e implementar el conocimiento en una amplia gama de tareas, como lo har√≠a un humano.  A√∫n no existe.\n",
      "    *   **Superinteligencia (Artificial Superintelligence - ASI):**  Una IA que supera la inteligencia humana en todos los aspectos, incluyendo la creatividad, la resoluci√≥n de problemas y el conocimiento general.  Es un concepto te√≥rico y objeto de debate.\n",
      "\n",
      "*   **T√©cnicas y Enfoques:** La IA utiliza diversas t√©cnicas y enfoques, entre los que destacan:\n",
      "    *   **Aprendizaje Autom√°tico (Machine Learning):**  Permite a las m√°quinas aprender de los datos sin ser programadas expl√≠citamente.\n",
      "    *   **Aprendizaje Profundo (Deep Learning):**  Un subconjunto del aprendizaje autom√°tico que utiliza redes neuronales artificiales con m√∫ltiples capas para analizar datos complejos.\n",
      "    *   **Procesamiento del Lenguaje Natural (NLP):**  Se centra en la interacci√≥n entre las computadoras y el lenguaje humano.\n",
      "    *   **Visi√≥n Artificial (Computer Vision):**  Permite a las m√°quinas \"ver\" e interpretar im√°genes y videos.\n",
      "    *   **Rob√≥tica:**  Dise√±o, construcci√≥n, operaci√≥n y aplicaci√≥n de robots.\n",
      "    *   **Sistemas Expertos:**  Programas que imitan la capacidad de toma de decisiones de un experto humano en un campo espec√≠fico.\n",
      "\n",
      "*   **Aplicaciones de la IA:** La IA se est√° aplicando en una amplia gama de industrias y √°reas, incluyendo:\n",
      "    *   **Salud:** Diagn√≥stico de enfermedades, desarrollo de f√°rmacos, asistencia virtual a pacientes.\n",
      "    *   **Finanzas:** Detecci√≥n de fraudes, an√°lisis de riesgos, asesoramiento financiero automatizado.\n",
      "    *   **Transporte:** Veh√≠culos aut√≥nomos, optimizaci√≥n de rutas, gesti√≥n del tr√°fico.\n",
      "    *   **Manufactura:** Automatizaci√≥n de procesos, control de calidad, mantenimiento predictivo.\n",
      "    *   **Educaci√≥n:** Tutor√≠a personalizada, evaluaci√≥n automatizada, creaci√≥n de contenido educativo.\n",
      "    *   **Entretenimiento:** Recomendaci√≥n de contenido, creaci√≥n de m√∫sica y arte, juegos.\n",
      "    *   **Atenci√≥n al Cliente:** Chatbots, asistentes virtuales, soporte t√©cnico automatizado.\n",
      "\n",
      "*   **Consideraciones √âticas:** El desarrollo y la implementaci√≥n de la IA plantean importantes cuestiones √©ticas, como:\n",
      "    *   **Sesgo:** La IA puede perpetuar y amplificar los sesgos presentes en los datos de entrenamiento.\n",
      "    *   **Privacidad:** La recopilaci√≥n y el uso de datos personales por parte de los sistemas de IA pueden plantear problemas de privacidad.\n",
      "    *   **Responsabilidad:** Es importante determinar qui√©n es responsable de las decisiones tomadas por los sistemas de IA.\n",
      "    *   **Empleo:** La automatizaci√≥n impulsada por la IA puede tener un impacto en el empleo.\n",
      "    *   **Seguridad:** Es crucial garantizar que los sistemas de IA sean seguros y no se utilicen para fines maliciosos.\n",
      "\n",
      "En resumen, la Inteligencia Artificial es un campo en constante evoluci√≥n que busca crear m√°quinas capaces de pensar y actuar de manera inteligente.  Tiene el potencial de transformar muchos aspectos de nuestras vidas, pero tambi√©n plantea importantes desaf√≠os √©ticos y sociales que deben abordarse de manera responsable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from openai import OpenAI\n",
    "\n",
    "# Config Gemini usando el endpoint OpenAI-compatible\n",
    "gemini = OpenAI(\n",
    "    api_key=gemini_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "prompt = \"¬øQu√© es la inteligencia artificial?\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"gemini_tracking\"):\n",
    "    # Registrar par√°metros\n",
    "    mlflow.log_params({\n",
    "        \"provider\": \"gemini\",\n",
    "        \"model_name\": model_name,\n",
    "        \"temperature\": 0.0\n",
    "    })\n",
    "\n",
    "    # Medir latencia\n",
    "    t0 = time.perf_counter()\n",
    "    response = gemini.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    latency_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Registrar m√©tricas\n",
    "    mlflow.log_metric(\"latency_ms\", latency_ms)\n",
    "    mlflow.log_metric(\"prompt_tokens\", getattr(response.usage, \"prompt_tokens\", 0))\n",
    "    mlflow.log_metric(\"completion_tokens\", getattr(response.usage, \"completion_tokens\", 0))\n",
    "\n",
    "    # Guardar la salida como artefacto\n",
    "    with open(\"gemini_output.txt\", \"w\") as f:\n",
    "        f.write(f\"Prompt: {prompt}\\n\\nRespuesta: {answer}\")\n",
    "    mlflow.log_artifact(\"gemini_output.txt\")\n",
    "\n",
    "print(\"‚úÖ GEMINI:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929b98f",
   "metadata": {},
   "source": [
    "## Tracker de Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91b49cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini(prompt: str, temperature: float = 0.2):\n",
    "    t0 = time.time()\n",
    "    response = gemini.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    latency = time.time() - t0\n",
    "    text = response.choices[0].message.content\n",
    "    tokens = len((prompt + \" \" + text).split())  # simulaci√≥n simple de tokens\n",
    "    return {\"response\": text, \"latency\": latency, \"tokens\": tokens}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edf5fc",
   "metadata": {},
   "source": [
    "# Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "478d9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run ollama_tracking at: http://127.0.0.1:5000/#/experiments/539662435075150292/runs/c3f2955be69446aa9dc018055bd375bd\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/539662435075150292\n",
      "‚úÖ OLLAMA: MLflow es una plataforma de ciencia de datos y aprendizaje autom√°tico que permite a los desarrolladores y cient√≠ficos de datos crear, entrenar y depurar modelos de machine learning de manera eficiente y escalable.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# Cliente OpenAI apuntando al endpoint local de Ollama\n",
    "ollama = OpenAI(\n",
    "    api_key=\"ollama\",  # obligatorio, aunque Ollama no lo usa\n",
    "    base_url=\"http://localhost:11434/v1\"\n",
    ")\n",
    "model_name = \"llama3.2\"\n",
    "prompt = \"Explica qu√© es MLflow en una frase.\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"ollama_tracking\"):\n",
    "    mlflow.log_params({\n",
    "        \"provider\": \"ollama\",\n",
    "        \"model_name\": model_name,\n",
    "        \"temperature\": 0.0,\n",
    "        \n",
    "    })\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    latency_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    mlflow.log_metric(\"latency_ms\", latency_ms)\n",
    "\n",
    "    with open(\"ollama_output.txt\", \"w\") as f:\n",
    "        f.write(f\"Prompt: {prompt}\\n\\nRespuesta: {answer}\")\n",
    "    mlflow.log_artifact(\"ollama_output.txt\")\n",
    "\n",
    "print(\"‚úÖ OLLAMA:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966bb6c",
   "metadata": {},
   "source": [
    "## Tracker de Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b210f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ollama(prompt: str, temperature: float = 0.0):\n",
    "    t0 = time.time()\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    latency = time.time() - t0\n",
    "    text = response.choices[0].message.content\n",
    "    tokens = len((prompt + \" \" + text).split())\n",
    "    return {\"response\": text, \"latency\": latency, \"tokens\": tokens}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da443cd2",
   "metadata": {},
   "source": [
    "## Registro de ejecuciones de LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7ccb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_llm(provider, model_name, fn, prompt, temperature=0.2, cost_per_token=0.000002):\n",
    "    with mlflow.start_run(run_name=f\"{provider}_run\"):\n",
    "        result = fn(prompt, temperature)\n",
    "        \n",
    "        # Log params\n",
    "        mlflow.log_param(\"provider\", provider)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        mlflow.log_param(\"task\", \"chat\")\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"latency_s\", result[\"latency\"])\n",
    "        mlflow.log_metric(\"total_tokens\", result[\"tokens\"])\n",
    "        mlflow.log_metric(\"estimated_cost\", result[\"tokens\"] * cost_per_token)\n",
    "\n",
    "        # Log artifacts (prompt y respuesta)\n",
    "        with open(\"prompt.txt\", \"w\") as f:\n",
    "            f.write(prompt)\n",
    "        with open(\"response.txt\", \"w\") as f:\n",
    "            f.write(result[\"response\"])\n",
    "\n",
    "        mlflow.log_artifact(\"prompt.txt\", artifact_path=\"prompts\")\n",
    "        mlflow.log_artifact(\"response.txt\", artifact_path=\"responses\")\n",
    "\n",
    "        print(f\"{provider} registrado en MLflow ({model_name})\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f968c5",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n de ambos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7bac54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini registrado en MLflow (llm_gemini_chat)\n",
      "üèÉ View run gemini_run at: http://127.0.0.1:5000/#/experiments/539662435075150292/runs/0e39ccf1ddb64f7cb658d4fb9d474f1b\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/539662435075150292\n",
      "ollama registrado en MLflow (llm_ollama_chat)\n",
      "üèÉ View run ollama_run at: http://127.0.0.1:5000/#/experiments/539662435075150292/runs/9c7090e1d2f74d66813c3ac895b9c830\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/539662435075150292\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explica brevemente qu√© hace MLflow y por qu√© es √∫til para modelos de lenguaje.\"\n",
    "\n",
    "gemini_result = track_llm(\"gemini\", \"llm_gemini_chat\", generate_gemini, prompt, temperature=0.2)\n",
    "ollama_result = track_llm(\"ollama\", \"llm_ollama_chat\", generate_ollama, prompt, temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ae582",
   "metadata": {},
   "source": [
    "## Registro de ambos modelos en el model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08010290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n",
      "2025/10/29 13:20:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/10/29 13:20:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'llm_gemini_chat' already exists. Creating a new version of this model...\n",
      "2025/10/29 13:20:51 WARNING mlflow.tracking._model_registry.fluent: Run with id be86ea47b8e040e8a6823e88067faa68 has no artifacts at artifact path 'model', registering model based on models:/m-4d24717049574df1b6b07589c51b2a6a instead\n",
      "Created version '3' of model 'llm_gemini_chat'.\n",
      "2025/10/29 13:20:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 'llm_gemini_chat' registrado correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/10/29 13:20:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'llm_ollama_chat' already exists. Creating a new version of this model...\n",
      "2025/10/29 13:20:55 WARNING mlflow.tracking._model_registry.fluent: Run with id ca6fb8e9047f4afa845f83e6bedda8e9 has no artifacts at artifact path 'model', registering model based on models:/m-372020f553c24026a0b132d20b634576 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 'llm_ollama_chat' registrado correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'llm_ollama_chat'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# Configurar el tracking local en la carpeta mlruns\n",
    "mlflow.set_tracking_uri(\"file://\" + os.path.abspath(\"mlruns\"))\n",
    "\n",
    "# Crear (o usar) el experimento\n",
    "mlflow.set_experiment(\"genai_model_registry\")\n",
    "\n",
    "# Crear un modelo simulado compatible con PyFunc\n",
    "class DummyModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        return [\"respuesta generada\"]\n",
    "\n",
    "# --- Registrar GEMINI ---\n",
    "with mlflow.start_run(run_name=\"registro_llm_gemini\") as run_gemini:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=DummyModel()\n",
    "    )\n",
    "    model_uri_gemini = f\"runs:/{run_gemini.info.run_id}/model\"\n",
    "    mlflow.register_model(model_uri=model_uri_gemini, name=\"llm_gemini_chat\")\n",
    "    print(\"Modelo 'llm_gemini_chat' registrado correctamente.\")\n",
    "\n",
    "# --- Registrar OLLAMA ---\n",
    "with mlflow.start_run(run_name=\"registro_llm_ollama\") as run_ollama:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=DummyModel()\n",
    "    )\n",
    "    model_uri_ollama = f\"runs:/{run_ollama.info.run_id}/model\"\n",
    "    mlflow.register_model(model_uri=model_uri_ollama, name=\"llm_ollama_chat\")\n",
    "    print(\"Modelo 'llm_ollama_chat' registrado correctamente.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744971a8",
   "metadata": {},
   "source": [
    "## Promovemos los modelos a Stagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67fd9924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/gxwhgd3s2v73lfrd296fzzs40000gn/T/ipykernel_74397/3515754190.py:11: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  versions = client.get_latest_versions(name)\n",
      "/var/folders/69/gxwhgd3s2v73lfrd296fzzs40000gn/T/ipykernel_74397/3515754190.py:14: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ llm_gemini_chat v1 promovido a Staging\n",
      "‚úÖ llm_ollama_chat v2 promovido a Staging\n"
     ]
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# Crear cliente\n",
    "client = MlflowClient()\n",
    "\n",
    "# Lista de modelos a promover\n",
    "model_names = [\"llm_gemini_chat\", \"llm_ollama_chat\"]\n",
    "\n",
    "for name in model_names:\n",
    "    try:\n",
    "        versions = client.get_latest_versions(name)\n",
    "        if versions:\n",
    "            version = versions[0].version\n",
    "            client.transition_model_version_stage(\n",
    "                name=name,\n",
    "                version=version,\n",
    "                stage=\"Staging\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(f\"‚úÖ {name} v{version} promovido a Staging\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No se encontraron versiones para {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al promover {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28218194",
   "metadata": {},
   "source": [
    "## Comparaci√≥n de m√©tricas entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "820d8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Comparaci√≥n de modelos Gemini vs Ollama:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ejecuci√≥n</th>\n",
       "      <th>Proveedor</th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Temperatura</th>\n",
       "      <th>Latencia (s)</th>\n",
       "      <th>Tokens totales</th>\n",
       "      <th>Costo estimado ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ollama_run</td>\n",
       "      <td>ollama</td>\n",
       "      <td>llm_ollama_chat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.257168</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini_run</td>\n",
       "      <td>gemini</td>\n",
       "      <td>llm_gemini_chat</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.125844</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.000582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ollama_tracking</td>\n",
       "      <td>ollama</td>\n",
       "      <td>llama3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini_tracking</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ollama_run</td>\n",
       "      <td>ollama</td>\n",
       "      <td>llm_ollama_chat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.061068</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini_run</td>\n",
       "      <td>gemini</td>\n",
       "      <td>llm_gemini_chat</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.600256</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ollama_tracking</td>\n",
       "      <td>ollama</td>\n",
       "      <td>llama3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gemini_tracking</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Ejecuci√≥n Proveedor            Modelo Temperatura  Latencia (s)  \\\n",
       "0       ollama_run    ollama   llm_ollama_chat         0.0     47.257168   \n",
       "1       gemini_run    gemini   llm_gemini_chat         0.2     13.125844   \n",
       "2  ollama_tracking    ollama          llama3.2         0.0           NaN   \n",
       "3  gemini_tracking    gemini  gemini-2.0-flash         0.0           NaN   \n",
       "4       ollama_run    ollama   llm_ollama_chat         0.0     51.061068   \n",
       "5       gemini_run    gemini   llm_gemini_chat         0.2      4.600256   \n",
       "6  ollama_tracking    ollama          llama3.2         0.0           NaN   \n",
       "7  gemini_tracking    gemini  gemini-2.0-flash         0.0           NaN   \n",
       "\n",
       "   Tokens totales  Costo estimado ($)  \n",
       "0           225.0            0.000450  \n",
       "1           291.0            0.000582  \n",
       "2             NaN                 NaN  \n",
       "3             NaN                 NaN  \n",
       "4           225.0            0.000450  \n",
       "5           337.0            0.000674  \n",
       "6             NaN                 NaN  \n",
       "7             NaN                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Promedio de m√©tricas por proveedor:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latencia (s)</th>\n",
       "      <th>Costo estimado ($)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proveedor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>8.8630</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ollama</th>\n",
       "      <td>49.1591</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Latencia (s)  Costo estimado ($)\n",
       "Proveedor                                  \n",
       "gemini           8.8630              0.0006\n",
       "ollama          49.1591              0.0004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "\n",
    "mlflow.set_tracking_uri(\"file://\" + os.path.abspath(\"mlruns\")) # Usando la de archivos como en tu notebook\n",
    "client = MlflowClient()\n",
    "\n",
    "# 1. APUNTAR AL EXPERIMENTO CORRECTO\n",
    "experiment = mlflow.get_experiment_by_name(\"llm_genai_model_registry\")\n",
    "\n",
    "# Obtener todos los runs del experimento\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "# 2. USAR LOS NOMBRES CORRECTOS DE LAS COLUMNAS\n",
    "# Estos son los nombres reales que registraste con track_llm\n",
    "cols = [\n",
    "    \"tags.mlflow.runName\",\n",
    "    \"params.provider\",\n",
    "    \"params.model_name\",\n",
    "    \"params.temperature\",\n",
    "    \"metrics.latency_s\",\n",
    "    \"metrics.total_tokens\",\n",
    "    \"metrics.estimated_cost\"\n",
    "]\n",
    "\n",
    "# Filtrar y renombrar columnas\n",
    "existing_cols = [c for c in cols if c in runs_df.columns]\n",
    "comparison = runs_df[existing_cols].rename(columns={\n",
    "    \"tags.mlflow.runName\": \"Ejecuci√≥n\",\n",
    "    \"params.provider\": \"Proveedor\",\n",
    "    \"params.model_name\": \"Modelo\",\n",
    "    \"params.temperature\": \"Temperatura\",\n",
    "    \"metrics.latency_s\": \"Latencia (s)\",\n",
    "    \"metrics.total_tokens\": \"Tokens totales\",\n",
    "    \"metrics.estimated_cost\": \"Costo estimado ($)\"\n",
    "})\n",
    "\n",
    "# Mostrar comparaci√≥n\n",
    "print(\"üìä Comparaci√≥n de modelos Gemini vs Ollama:\")\n",
    "display(comparison)\n",
    "\n",
    "# Mostrar resumen comparativo promedio\n",
    "if not comparison.empty and \"Proveedor\" in comparison.columns:\n",
    "    resumen = comparison.groupby(\"Proveedor\")[[\"Latencia (s)\", \"Costo estimado ($)\"]].mean().round(4)\n",
    "    print(\"\\nüìà Promedio de m√©tricas por proveedor:\")\n",
    "    display(resumen)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudieron generar las m√©tricas. Revisa los nombres del experimento y las columnas.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
